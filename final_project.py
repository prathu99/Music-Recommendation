# -*- coding: utf-8 -*-
"""final-priject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J9fdbl2uPQG8236fWiI3F5sOwA5QBEWj
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns



from google.colab import files
uploaded=files.upload()



df=pd.read_csv("data.csv")
df.head(10)

#null values
pd.isnull(df).sum()

df.info()

popular_df=df.sort_values("popularity",ascending=True).head(10)
popular_df

df.describe().transpose()

most_popular=df.query("popularity>90",inplace=False).sort_values("popularity",ascending = False)
most_popular[:10]

df[["artists"]].iloc[16]

df["duration"]=df["duration_ms"].apply(lambda x: round(x/1000))
df.drop("duration_ms", inplace=True, axis=1)
df.head()

df.duration.head()

corr_df=df.drop(["key","mode","explicit"],axis=1).corr(method="pearson")
plt.figure(figsize=(14,6))
heatmap=sns.heatmap(corr_df,annot=True,fmt=".1g",vmin=-1, vmax=1, center=0, cmap="viridis", linewidths=1, linecolor="Blue")
heatmap.set_title("Correlation HeatMap Between Variable")
heatmap.set_xticklabels(heatmap.get_xticklabels(),rotation=90)
plt.show()

sample_df=df.sample(int(0.04*len(df)))

print(len(sample_df))

plt.figure(figsize=(10,6))
sns.regplot(data=sample_df,y="loudness", x="energy" , color="c").set(title="Loudness vs Energy correlation")
plt.show()

plt.figure(figsize=(10,6))
sns.regplot(data=sample_df,y="popularity", x="acousticness" , color="b").set(title="popularity vs acousticness correlation")
plt.show()

plt.figure(figsize=(18, 10))
sns.histplot(data=df, x='year', discrete=True, stat='count', kde=False)
plt.title("Number of Music Entries per Year (Histogram)")
plt.xlabel("Year")
plt.ylabel("Count")
plt.xticks(rotation=90)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

total_dr = df['duration']  # Assuming 'duration' is a column in your DataFrame
years = df['year']  # Assuming 'year' is a column in your DataFrame
fig_dims = (18, 7)

fig, ax = plt.subplots(figsize=fig_dims)
sns.barplot(x=years, y=total_dr, ax=ax, errwidth=False)
ax.set_title("Years vs Duration")  # Use ax.set_title() to set the title
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)  # Correct the typo here
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

total_dr = df['duration']  # Assuming 'duration' is a column in your DataFrame
years = df['year']  # Assuming 'year' is a column in your DataFrame
sns.set_style(style="whitegrid")
fig_dims = (10,5 )

fig, ax = plt.subplots(figsize=fig_dims)
sns.lineplot(x=years, y=total_dr, ax=ax)
ax.set_title("Years vs Duration")  # Use ax.set_title() to set the title
ax.set_xticklabels(ax.get_xticklabels(), rotation=60)  # Correct the typo here
plt.show()

# Feature selection
features = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'loudness', 'speechiness', 'tempo', 'duration']

# Create a new DataFrame with only the selected features
song_features = df[features]

# Normalize the features (optional, but recommended)
song_features_normalized = (song_features - song_features.mean()) / (song_features.max() - song_features.min())

from sklearn.cluster import KMeans
# Fit K-means clustering
kmeans = KMeans(n_clusters=10, random_state=42)  # You can adjust the number of clusters as needed
kmeans.fit(song_features_normalized)

# Function to get song recommendations from the same cluster
def get_recommendations(song_name, kmeans_model=kmeans, data=df):
    song_index = df[df['name'] == song_name].index[0]
    cluster_label = kmeans_model.labels_[song_index]
    cluster_songs = df[kmeans_model.labels_ == cluster_label]['name']
    return cluster_songs

# Example: Get recommendations for a song
recommended_songs = get_recommendations("Clancy Lowered the Boom")
print(recommended_songs)

# Feature selection
features = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'loudness', 'speechiness', 'tempo', 'duration']

# Create a new DataFrame with only the selected features
X = df[features]
y = df['popularity']

from sklearn.ensemble import RandomForestRegressor
# Train a Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X, y)

# Function to get song recommendations based on predicted popularity
def get_recommendations(song_name, model=rf_model, data=df):
    song_features = df[df['name'] == song_name][features]
    predicted_popularity = model.predict(song_features)
    similar_songs = df[df['popularity'] >= predicted_popularity[0]].sort_values(by='popularity', ascending=False)
    return similar_songs['name']

# Example: Get recommendations for a song
recommended_songs = get_recommendations("Clancy Lowered the Boom")
print(recommended_songs)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

# Data preprocessing
user_encoder = LabelEncoder()
df['user_id'] = user_encoder.fit_transform(df['artists'])  # Assuming 'artists' is the user column

item_encoder = LabelEncoder()
df['item_id'] = item_encoder.fit_transform(df['name'])  # Assuming 'name' is the item column

scaler = MinMaxScaler()
df['rating'] = scaler.fit_transform(df['popularity'].values.reshape(-1, 1))

# Split the data into training and testing sets
train, test = train_test_split(df, test_size=0.2, random_state=42)

# Define the embedding size
embedding_dim = 32

# Define the number of unique users and items
num_users = df['user_id'].nunique()
num_items = df['item_id'].nunique()

# Input layers
user_input = Input(shape=(1,), name='user_input')
item_input = Input(shape=(1,), name='item_input')

# Embedding layers
user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, input_length=1, name='user_embedding')(user_input)
item_embedding = Embedding(input_dim=num_items, output_dim=embedding_dim, input_length=1, name='item_embedding')(item_input)

# Flatten the embeddings
user_flat = Flatten()(user_embedding)
item_flat = Flatten()(item_embedding)

# Concatenate user and item embeddings
concat = Concatenate()([user_flat, item_flat])

# Neural Collaborative Filtering (NCF) layers
fc1 = Dense(64, activation='relu', kernel_regularizer=l2(0.04))(concat)
fc2 = Dense(32, activation='relu', kernel_regularizer=l2(0.04))(fc1)
output = Dense(1, activation='sigmoid')(fc2)

# Create the NCF model
ncf_model = Model(inputs=[user_input, item_input], outputs=output)

# Compile the model
ncf_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01), metrics=['accuracy'])

from tensorflow.keras.callbacks import LearningRateScheduler

# Define a learning rate scheduler
def lr_scheduler(epoch, lr):
    if epoch < 5:
        return lr  # Keep the initial learning rate for the first 5 epochs
    else:
        return lr * 0.95  # Reduce the learning rate by 5% after each epoch

# Create a learning rate scheduler callback
lr_callback = LearningRateScheduler(lr_scheduler)

# Train the model
ncf_model.fit([train['user_id'], train['item_id']], train['rating'], batch_size=64, epochs=10, verbose=1)

# Evaluate the model on the test set
test_loss, test_accuracy = ncf_model.evaluate([test['user_id'], test['item_id']], test['rating'], verbose=0)
print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')

# Function to get song recommendations for a user
def get_song_recommendations(user_id, n=10):
    # Create a list of all item IDs
    all_item_ids = np.arange(num_items)

    # Repeat the user ID for all items
    user_ids = np.full(num_items, user_id)

    # Predict ratings for all user-item pairs
    ratings = ncf_model.predict([user_ids, all_item_ids])

    # Create a DataFrame to store item IDs and their predicted ratings
    predictions_df = pd.DataFrame({'item_id': all_item_ids, 'predicted_rating': ratings[:, 0]})

    # Sort items by predicted ratings in descending order
    top_n_recommendations = predictions_df.sort_values(by='predicted_rating', ascending=False).head(n)

    # Map item IDs back to their original values
    top_n_recommendations['item_name'] = item_encoder.inverse_transform(top_n_recommendations['item_id'])

    return top_n_recommendations[['item_name', 'predicted_rating']]

# Example: Get song recommendations for a user (replace user_id with the actual user ID)
user_id = 123 # Replace with the actual user ID
recommendations = get_song_recommendations(user_id, n=10)
print("Recommended Songs for User:")
print(recommendations)

